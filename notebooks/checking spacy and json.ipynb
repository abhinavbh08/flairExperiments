{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a68ab9d01a0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgold\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbiluo_tags_from_offsets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.gold import biluo_tags_from_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will first read the data nd then convert it into spacy format. After that I will extract BILOU tags from it and then convert it in to specific format required by flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1, y2 = 150, 160\n",
    "x1, x2 = 145, 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1<=y2 and y1<=x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = 150, 160\n",
    "y1, y2 = 145, 149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "import spacy\n",
    "from spacy.gold import biluo_tags_from_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-18 18:17:48,886 loading file C:\\Users\\hp\\.flair\\models\\en-ner-conll03-v0.4.pt\n",
      "I love Berlin! <S-PER>\n",
      "TAGS->>>>>>>>>>>.. ['O', 'O', 'B-PERSON', 'L-PERSON', 'O']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-5d940c4e6ce7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mcheck_ner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-5d940c4e6ce7>\u001b[0m in \u001b[0;36mcheck_ner\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbiluo_tags_from_offsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'entities'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TAGS->>>>>>>>>>>..\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TEXT->>>>>>>>>>\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'tags'"
     ]
    }
   ],
   "source": [
    "def check_ner():\n",
    "    tagger = SequenceTagger.load('ner')\n",
    "    sentence = Sentence('I love Berlin!')\n",
    "    tagger.predict(sentence)\n",
    "    print(sentence.to_tagged_string())\n",
    "\n",
    "\n",
    "    TRAIN_DATA = [\n",
    "        (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n",
    "        (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOCSEX\"), (18, 24, \"LOCSEX\")]}),\n",
    "    ]\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    docs = []\n",
    "    for text, annot in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        tags = biluo_tags_from_offsets(doc, annot['entities'])\n",
    "        print(\"TAGS->>>>>>>>>>>..\", tags)\n",
    "        print(\"TEXT->>>>>>>>>>\", doc.)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_ner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAGS->>>>>>>>>>>.. ['O', 'O', 'B-PERSON', 'L-PERSON', 'O']\n",
      "TEXT->>>>>>>>>> ['Who', 'is', 'Shaka', 'Khan', '?']\n",
      "TAGS->>>>>>>>>>>.. ['O', 'O', 'U-LOCSEX', 'O', 'U-LOCSEX', 'O']\n",
      "TEXT->>>>>>>>>> ['I', 'like', 'London', 'and', 'Berlin', '.']\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = [\n",
    "    (\"Who is Shaka Khan?\", {\"entities\": [(7, 17, \"PERSON\")]}),\n",
    "    (\"I like London and Berlin.\", {\"entities\": [(7, 13, \"LOCSEX\"), (18, 24, \"LOCSEX\")]}),\n",
    "]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "docs = []\n",
    "\n",
    "for text, annot in TRAIN_DATA:\n",
    "    tokens = []\n",
    "    doc = nlp(text)\n",
    "    tags = biluo_tags_from_offsets(doc, annot['entities'])\n",
    "    for token in doc:\n",
    "        tokens.append(token.text)\n",
    "    print(\"TAGS->>>>>>>>>>>..\", tags)\n",
    "    print(\"TEXT->>>>>>>>>>\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['O', 'O', 'O', 'O', 'U-protein', 'O', 'O', 'O', 'O', 'O', 'O', 'B-assay', 'I-assay', 'I-assay', 'I-assay', 'L-assay', 'O', 'O', 'O', 'B-experimental-construct', 'I-experimental-construct', 'I-experimental-construct', 'I-experimental-construct', 'L-experimental-construct', 'O', 'O', 'O', 'O', 'O', 'O', 'U-chemical', 'O', 'O', 'O', 'O', 'O', 'O', 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tags = []\n",
    "for tag in tags:\n",
    "    if tag[0]==\"U\":\n",
    "        new_tags.append(\"B\"+tag[1:])\n",
    "    elif tag[0]==\"L\":\n",
    "        new_tags.append(\"I\"+tag[1:])\n",
    "    else:\n",
    "        new_tags.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-protein',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-assay',\n",
       " 'I-assay',\n",
       " 'I-assay',\n",
       " 'I-assay',\n",
       " 'I-assay',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-experimental-construct',\n",
       " 'I-experimental-construct',\n",
       " 'I-experimental-construct',\n",
       " 'I-experimental-construct',\n",
       " 'I-experimental-construct',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-chemical',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [([\"Abhinav\", \"Bhatt\", \",\", \"lamp\"], [\"name\", \"name\", \"O\", \"O\"]), ([\"Najsgd\", \"Shweta\", \"Anil\"], [\"name\", \"Date\", \"O\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abhinav', 'Bhatt', ',', 'lamp'] ['name', 'name', 'O', 'O']\n",
      "['Najsgd', 'Shweta', 'Anil'] ['name', 'Date', 'O']\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc[0], doc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"file.txt\", \"w\") as file:\n",
    "    for doc in docs:\n",
    "        for name, label in zip(doc[0], doc[1]):\n",
    "            file.write(name+\" \"+label+\"\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Note',\n",
       " ':',\n",
       " 'Since',\n",
       " 'the',\n",
       " 'fourteenth',\n",
       " 'century',\n",
       " 'the',\n",
       " 'practice',\n",
       " 'of',\n",
       " '“',\n",
       " 'medicine',\n",
       " '”',\n",
       " 'has',\n",
       " 'become',\n",
       " 'a',\n",
       " 'profession',\n",
       " ';',\n",
       " 'and',\n",
       " 'more',\n",
       " 'importantly',\n",
       " ',',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'male-dominated',\n",
       " 'profession',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_tokenizer(nlp):\n",
    "    infix_re = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
    "    prefix_re = compile_prefix_regex(nlp.Defaults.prefixes)\n",
    "#     suffix_re = re.compile('…$|……$|,$|:$|;$|\\\\!$|\\\\?$|¿$|؟$|¡$|\\\\($|\\\\)$|\\\\[$|\\\\]$|\\\\{$|\\\\}$|<$|>$|_$|#$|\\\\*$|&$|。$|？$|！$|，$|、$|；$|：$|～$|·$|।$|،$|۔$|؛$|٪$|\\\\.\\\\.+$|…$|\\\\\\'$|\"$|”$|“$|`$|‘$|´$|’$|‚$|,$|„$|»$|«$|「$|」$|『$|』$|（$|）$|)\n",
    "    suffix_re = compile_suffix_regex(nlp.Defaults.suffixes)\n",
    "    return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer,\n",
    "                                token_match=None)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "\n",
    "doc = nlp(u'Note: Since the fourteenth century the practice of “medicine” has become a profession; and more importantly, it\\'s a male-dominated profession.')\n",
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'world', '(', 'is', ')', 'dange', 'CA2+', 'stuff', 'hyphen-comes', 'cant', \"'\", 't', 'do', 'this', 'tihing']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "prefix_re = re.compile(r'''^[[(\"']''')\n",
    "suffix_re = re.compile(r'''[])\"']$''')\n",
    "infix_re = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
    "# simple_url_re = re.compile(r'''[a-zA-Z0-9]/+''')\n",
    "\n",
    "def custom_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab,\n",
    "                                prefix_search=prefix_re.search,\n",
    "                                suffix_search=suffix_re.search,\n",
    "                                infix_finditer=infix_re.finditer)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer = custom_tokenizer(nlp)\n",
    "doc = nlp(\"This world (is) dange CA2+ stuff hyphen-comes cant't do this tihing\")\n",
    "print([t.text for t in doc]) # ['hello', '-', 'world.', ':)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'world',\n",
       " '(',\n",
       " 'is',\n",
       " ')',\n",
       " 'dange',\n",
       " 'CA2+',\n",
       " 'stuff',\n",
       " 'hyphen-comes',\n",
       " 'cant',\n",
       " \"'\",\n",
       " 't',\n",
       " 'do',\n",
       " 'this',\n",
       " 'tihing']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc = nlp(\"Briefly, the oligonucleotide library was incubated with fusion proteins bound to either GST-coated bead-bound fusion proteins (GST-tag) or nickel/nitrilotriacetic acid matrix-bound fusion proteins (histidine-tag) in a buffer containing 20 mM Tris (pH 8.0), 50 mM KCl, 1 mM DTT, 0.5 mM EDTA, 10% glycerol, 20 μg/ml BSA, and 2 μg/ml poly(dI)⋅poly(dc).\")\n",
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " ' ',\n",
       " 'calmodulin',\n",
       " 'is',\n",
       " 'added',\n",
       " ',',\n",
       " 'the',\n",
       " 'inhibition',\n",
       " 'of',\n",
       " ' ',\n",
       " 'fascin-actin',\n",
       " 'interaction',\n",
       " 'by',\n",
       " ' ',\n",
       " 'caldesmon',\n",
       " 'and',\n",
       " ' ',\n",
       " 'TM',\n",
       " 'becomes',\n",
       " 'Ca2+',\n",
       " 'dependent',\n",
       " 'because',\n",
       " ' ',\n",
       " 'Ca2+/calmodulin',\n",
       " 'blocks',\n",
       " ' ',\n",
       " 'actin',\n",
       " 'binding',\n",
       " 'of',\n",
       " ' ',\n",
       " 'caldesmon',\n",
       " '.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"When  calmodulin is added, the inhibition of  fascin-actin interaction by  caldesmon and  TM becomes Ca2+ dependent because  Ca2+/calmodulin blocks  actin binding of  caldesmon.\")\n",
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-20 15:32:09,208 Reading data from .\n",
      "2020-01-20 15:32:09,214 Train: file_train.txt\n",
      "2020-01-20 15:32:09,216 Dev: file_train.txt\n",
      "2020-01-20 15:32:09,218 Test: None\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# define columns\n",
    "columns = {0: 'text', 1: 'ner'}\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = ''\n",
    "\n",
    "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file='file_train.txt',\n",
    "                             dev_file=\"file_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) We have demonstrated that tropomyosin <B-protein-family> and troponin <B-protein-complex> are actually bound to the actin <B-protein> filaments at motility assay conditions .\n"
     ]
    }
   ],
   "source": [
    "print(corpus.train[0].to_tagged_string('ner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "# from flair.datasets import WNUT_17\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, CharacterEmbeddings, FlairEmbeddings\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 1265 train + 1405 dev + 140 test sentences\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TRAIN\": {\n",
      "        \"dataset\": \"TRAIN\",\n",
      "        \"total_number_of_documents\": 1265,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 39145,\n",
      "            \"min\": 3,\n",
      "            \"max\": 154,\n",
      "            \"avg\": 30.944664031620555\n",
      "        }\n",
      "    },\n",
      "    \"TEST\": {\n",
      "        \"dataset\": \"TEST\",\n",
      "        \"total_number_of_documents\": 140,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 4468,\n",
      "            \"min\": 5,\n",
      "            \"max\": 95,\n",
      "            \"avg\": 31.914285714285715\n",
      "        }\n",
      "    },\n",
      "    \"DEV\": {\n",
      "        \"dataset\": \"DEV\",\n",
      "        \"total_number_of_documents\": 1405,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {},\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 43613,\n",
      "            \"min\": 3,\n",
      "            \"max\": 154,\n",
      "            \"avg\": 31.041281138790037\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.obtain_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ) We have demonstrated that tropomyosin <B-protein-family> and troponin <B-protein-complex> are actually bound to the actin <B-protein> filaments at motility assay conditions .\n",
      "1 ) We have demonstrated that tropomyosin and troponin are actually bound to the actin filaments at motility assay conditions .\n"
     ]
    }
   ],
   "source": [
    "print(corpus.train[0].to_tagged_string('ner'))\n",
    "print(corpus.train[0].to_tagged_string('pos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'<unk>', b'O', b'B-protein-family', b'B-protein-complex', b'B-protein', b'-', b'I-protein-family', b'B-protein-domain', b'I-protein-domain', b'B-chemical', b'B-DNA', b'I-DNA', b'B-mutation', b'B-protein-motif', b'B-cell', b'B-tissue', b'B-protein-isoform', b'B-experiment-tag', b'B-reagent', b'I-reagent', b'B-disease', b'I-protein-complex', b'B-protein-region', b'I-protein', b'B-experimental-construct', b'B-organelle', b'B-assay', b'I-cell', b'I-protein-motif', b'B-RNA', b'B-fusion-protein', b'B-peptide', b'I-peptide', b'B-gene', b'I-chemical', b'I-protein-isoform', b'B-amino-acid', b'I-fusion-protein', b'I-protein-region', b'I-tissue', b'B-drug', b'I-experimental-construct', b'I-amino-acid', b'I-assay', b'B-organism', b'B-process', b'I-organelle', b'I-RNA', b'I-mutation', b'B-parameter', b'I-experiment-tag', b'I-gene', b'B-brand', b'B-protein-RNA-complex', b'I-protein-RNA-complex', b'I-process', b'I-organism', b'I-brand', b'I-drug', b'I-disease', b'B-protein-DNA-complex', b'I-protein-DNA-complex', b'B-RNA-family', b'<START>', b'<STOP>']\n"
     ]
    }
   ],
   "source": [
    "tag_type = 'ner'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "print(tag_dictionary.idx2item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "print(len(tag_dictionary.idx2item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_types: List[TokenEmbeddings] = [\n",
    "\n",
    "#     WordEmbeddings('glove'),\n",
    "\n",
    "    # comment in this line to use character embeddings\n",
    "#     CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use flair embeddings.\n",
    "    FlairEmbeddings('news-forward'),\n",
    "    FlairEmbeddings('news-backward'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-20 15:32:16,192 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-20 15:32:16,193 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.05, inplace=False)\n",
      "        (encoder): Embedding(300, 100)\n",
      "        (rnn): LSTM(100, 2048)\n",
      "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (list_embedding_1): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.05, inplace=False)\n",
      "        (encoder): Embedding(300, 100)\n",
      "        (rnn): LSTM(100, 2048)\n",
      "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (rnn): LSTM(4096, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=65, bias=True)\n",
      ")\"\n",
      "2020-01-20 15:32:16,194 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-20 15:32:16,195 Corpus: \"Corpus: 1265 train + 1405 dev + 140 test sentences\"\n",
      "2020-01-20 15:32:16,197 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-20 15:32:16,199 Parameters:\n",
      "2020-01-20 15:32:16,200  - learning_rate: \"0.1\"\n",
      "2020-01-20 15:32:16,201  - mini_batch_size: \"16\"\n",
      "2020-01-20 15:32:16,202  - patience: \"3\"\n",
      "2020-01-20 15:32:16,203  - anneal_factor: \"0.5\"\n",
      "2020-01-20 15:32:16,204  - max_epochs: \"10\"\n",
      "2020-01-20 15:32:16,205  - shuffle: \"True\"\n",
      "2020-01-20 15:32:16,206  - train_with_dev: \"False\"\n",
      "2020-01-20 15:32:16,206  - batch_growth_annealing: \"False\"\n",
      "2020-01-20 15:32:16,208 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-20 15:32:16,209 Model training base path: \".\"\n",
      "2020-01-20 15:32:16,210 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-20 15:32:16,211 Device: cpu\n",
      "2020-01-20 15:32:16,212 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-20 15:32:16,213 Embeddings storage mode: cpu\n",
      "2020-01-20 15:32:16,215 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-20 15:33:13,218 epoch 1 - iter 8/80 - loss 50.19185233 - samples/sec: 2.25\n",
      "2020-01-20 15:34:06,032 epoch 1 - iter 16/80 - loss 40.51346910 - samples/sec: 2.42\n",
      "2020-01-20 15:35:02,435 epoch 1 - iter 24/80 - loss 36.09429979 - samples/sec: 2.27\n",
      "2020-01-20 15:36:07,633 epoch 1 - iter 32/80 - loss 33.17631006 - samples/sec: 1.96\n",
      "2020-01-20 15:37:08,024 epoch 1 - iter 40/80 - loss 31.49868422 - samples/sec: 2.12\n",
      "2020-01-20 15:38:07,867 epoch 1 - iter 48/80 - loss 29.78561769 - samples/sec: 2.14\n",
      "2020-01-20 15:39:02,837 epoch 1 - iter 56/80 - loss 28.77544294 - samples/sec: 2.33\n",
      "2020-01-20 15:40:01,150 epoch 1 - iter 64/80 - loss 27.83874059 - samples/sec: 2.20\n",
      "2020-01-20 15:40:56,154 epoch 1 - iter 72/80 - loss 26.86877947 - samples/sec: 2.33\n",
      "2020-01-20 15:41:43,617 epoch 1 - iter 80/80 - loss 26.16293383 - samples/sec: 2.70\n",
      "2020-01-20 15:41:43,633 ----------------------------------------------------------------------------------------------------\n",
      "2020-01-20 15:41:43,635 EPOCH 1 done: loss 26.1629 - lr 0.1000\n"
     ]
    }
   ],
   "source": [
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train('',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=16,\n",
    "              max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. plot weight traces (optional)\n",
    "from flair.visual.training_curves import Plotter\n",
    "plotter = Plotter()\n",
    "plotter.plot_weights('weights.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-19 11:08:29,817 loading file final-model.pt\n",
      "Sam Houston stayed home. <B-PER>\n"
     ]
    }
   ],
   "source": [
    "# load the model you trained\n",
    "from flair.data import Sentence\n",
    "\n",
    "model = SequenceTagger.load('final-model.pt')\n",
    "\n",
    "# create example sentence\n",
    "sentence = Sentence(\"Sam Houston stayed home.\")\n",
    "\n",
    "# predict tags and print\n",
    "model.predict(sentence)\n",
    "\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
