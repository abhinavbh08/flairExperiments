{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.gold import biluo_tags_from_offsets\n",
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/train-refactored.json\"\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1405"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_re = re.compile(r'''^[[(\"']''')\n",
    "suffix_re = re.compile(r'''[])\"']$''')\n",
    "infix_re = re.compile(r'''[.\\,\\?\\:\\;\\...\\‘\\’\\`\\“\\”\\\"\\'~]''')\n",
    "\n",
    "# simple_url_re = re.compile(r'''[a-zA-Z0-9]/+''')\n",
    "\n",
    "def create_tokenizer(nlp):\n",
    "    return Tokenizer(nlp.vocab,\n",
    "            rules={},\n",
    "            prefix_search=prefix_re.search,\n",
    "            suffix_search=suffix_re.search,\n",
    "            infix_finditer=infix_re.finditer\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp.tokenizer = create_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'the', 'good-thing', 'in', 'the', 'world', \"'\", 's', 'peace', '.']\n"
     ]
    }
   ],
   "source": [
    "# Testing the tokenizer\n",
    "doc = nlp(\"This is the good-thing in the world's peace.\")\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Moreover', ',', 'the', 'in', 'vitro', 'binding', 'of', 'NF-κB', 'or', 'Sp1', 'to', 'its', 'target', 'DNA', 'was', 'not', 'affected', 'by', 'the', 'presence', 'of', 'K-12', '.']\n",
      "['No', 'appreciable', 'binding', 'of', 'Ly49C', 'to', 'Db', 'could', 'be', 'detected', 'in', 'the', 'cell-cell', 'adhesion', 'assay', 'by', 'antibody', 'blocking', 'or', 'by', 'employing', 'cells', 'from', 'Kb', '/', 'mice', 'that', 'express', 'Db', 'normally', '(', 'Figure', '3', 'and', 'Figure', '4)', '.']\n",
      "['The', 'yeast', 'two-hybrid', 'interaction', 'assays', 'indicated', 'that', 'filamin-A', 'does', 'not', 'bind', 'to', 'any', 'integrin', 'β', 'subunit', ',', 'whereas', 'filamin-B', 'only', 'interacts', 'with', 'β1A', '.']\n",
      "[' ', 'Because', 'FLICE2', 'does', 'not', 'directly', 'bind', 'DR4', 'or', 'DR5', ',', 'a', 'yet', 'to', 'be', 'identified', 'adaptor', 'molecule', 'is', 'presumably', 'responsible', 'for', 'this', 'linkage', '.']\n",
      "['We', 'have', 'thus', 'examined', 'whether', 'actin', 'binding', 'of', 'fascin', 'can', 'be', 'regulated', 'in', 'a', 'calcium-dependent', 'way', 'by', 'controlling', 'actin', 'binding', 'of', 'caldesmon', 'with', 'Ca2+/calmodulin', '.']\n",
      "['Therefore', ',', 'we', 'cannot', 'rule', 'out', 'that', 'some', 'variations', 'in', 'the', 'binding', 'of', 'three', 'apoE', 'isotypes', 'to', 'Abeta', 'peptides', 'will', 'be', 'found', 'when', 'other', 'apoE', 'preparations', 'are', 'used', '.']\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for i, dataItem in enumerate(data):\n",
    "    tags = []\n",
    "    tokenized_text = [token.text for token in nlp(dataItem['text'])]\n",
    "    for textItem in tokenized_text:\n",
    "        flag = False\n",
    "        label = \"O\"\n",
    "        for slotItem in dataItem[\"slots\"]:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
